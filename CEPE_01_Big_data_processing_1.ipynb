{"cells":[{"cell_type":"markdown","source":["**Comment about functions that we have to import**\n* Have you find a rule about what we import : functions (col, expr, lit...) and objects (Row, StructField, StructType, StringType, LongType). We don't import methods belong to DF\n* count() is a excellent example to distinct transformation (need import) and action (don't need import)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f83dc5b-c1fb-461c-9f5b-56aba6978a30","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Schemas *(Addtionnal)*"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78d08230-43eb-40aa-81e1-732df381a020","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import Row\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\nmyManualSchema = StructType([\n    StructField(\"some\", StringType(), True),\n    StructField(\"col\", StringType(), True),\n    StructField(\"names\", LongType(), False)\n])\nmyRow = Row(\"Hello\", None, 1)\nmyDf = spark.createDataFrame([myRow], myManualSchema)\nmyDf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f89866e-82dd-46d8-8a6c-76dc301c6190","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 1. Data Sources, import and export\n#### CSV"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4889fe77-d4ff-4d12-9434-b31c647280eb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Let a data source define the schema\ndf = (spark.read\n      .format(\"csv\")\n      .option(\"header\", \"true\") # ajouter en seconde temps false default\n      .load(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv\")\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3911a5f3-f28c-40f6-ba9d-1953ffbd5d9e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show(2)\ndf.schema # poser la question d'attribut et method\ndf.printSchema() # ne pas oublier () de .printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64099c03-62e3-49eb-a206-81ca22db8e12","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\nOut[23]: StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', StringType(), True)])"]}],"execution_count":0},{"cell_type":"code","source":["# we can define it explicitly ourselves\nmyManualSchema = StructType([\n    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n    StructField(\"count\", LongType(), False)\n])\n\ndf = (spark.read\n      .format(\"csv\")\n      .option(\"header\", \"true\")\n      .schema(myManualSchema)\n      .load(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv\")\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9d26962-c970-42e0-a784-69703741cd59","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98f2edb1-f1cf-4444-94e9-e039ecf6fd02","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[15]: StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"]}],"execution_count":0},{"cell_type":"markdown","source":["**We can use the schemas of a existing DF**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"181ef7fc-3cbc-4c94-8591-5d1cca81b7db","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_2014 = (spark.read\n      .format(\"csv\")\n      .option(\"header\", \"true\")\n#       .schema(myManualSchema)\n      .load(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2014-summary.csv\")\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8fffcce-1127-4766-8443-bf729d7611bf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# We can used the schema of a existing df\ndf_2014 = (spark.read\n      .format(\"csv\")\n      .option(\"header\", \"true\")\n      .schema(df.schema)\n      .load(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2014-summary.csv\")\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c43ba117-011a-43d4-a309-0e6deee14477","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_2014.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86bd7ef0-2df2-487e-8ef5-5c0a71ac0b74","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[22]: StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Json\nJson have less options than csv does not have \"header\" option"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8a857fd-af8b-4241-ba23-9c8cf47c551a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_Json = (\n    spark.read.format(\"json\")\n#     .schema(df.schema) # there is no need to specify schema, json do better than csv\n    .load(\"/databricks-datasets/definitive-guide/data/flight-data/json/2015-summary.json\")\n)\n\ndf1 = spark.read.format(\"json\").load(\"/databricks-datasets/definitive-guide/data/flight-data/json/\") # with or without last /\ndf.count() # 256\ndf1.count() # 1502"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c11aba9-be5f-4341-9d4e-204e7c1c251c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_Json.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3e37a8a-1b1d-417d-afdf-29735f650a9d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["%who\ndel df1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90712479-6528-436c-83cc-419eb8747193","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["df\t df_Json\t os\t \n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Export and delete"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a171ed5-8d00-4de2-ba51-73874e610aff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.coalesce(1).write.format('csv').mode('overwrite').option('header', 'true').save('dbfs:/FileStore/df/export_fg.csv')\n# df.coalesce(1).write.format('csv').option('header', 'true').save('/dbfs/FileStore/NJ/wrtdftodbfs.txt')\n\n# list contents\ndbutils.fs.ls(\"FileStore/tables\")\n\n# delete element\ndbutils.fs.rm('FileStore/df/Sample.csv',True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02ccd6a7-0559-49aa-9e70-854555add314","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[65]: [FileInfo(path='dbfs:/FileStore/tables/2015_summary-1.json', name='2015_summary-1.json', size=21368, modificationTime=1679832308000),\n FileInfo(path='dbfs:/FileStore/tables/2015_summary-2.json', name='2015_summary-2.json', size=21368, modificationTime=1679832404000),\n FileInfo(path='dbfs:/FileStore/tables/2015_summary-3.json', name='2015_summary-3.json', size=21368, modificationTime=1679832512000),\n FileInfo(path='dbfs:/FileStore/tables/2015_summary.json', name='2015_summary.json', size=21368, modificationTime=1679831665000),\n FileInfo(path='dbfs:/FileStore/tables/AABA_2006_01_01_to_2018_01_01.csv', name='AABA_2006_01_01_to_2018_01_01.csv', size=145792, modificationTime=1675007933000),\n FileInfo(path='dbfs:/FileStore/tables/AMZN_2006_01_01_to_2018_01_01.csv', name='AMZN_2006_01_01_to_2018_01_01.csv', size=151374, modificationTime=1675008319000),\n FileInfo(path='dbfs:/FileStore/tables/GOOGL_2006_01_01_to_2018_01_01.csv', name='GOOGL_2006_01_01_to_2018_01_01.csv', size=158672, modificationTime=1674997195000),\n FileInfo(path='dbfs:/FileStore/tables/IBM_2006_01_01_to_2018_01_01.csv', name='IBM_2006_01_01_to_2018_01_01.csv', size=150445, modificationTime=1675008351000),\n FileInfo(path='dbfs:/FileStore/tables/artist_alias_small.txt', name='artist_alias_small.txt', size=9096, modificationTime=1678206607000),\n FileInfo(path='dbfs:/FileStore/tables/artist_data_small.txt', name='artist_data_small.txt', size=728192, modificationTime=1678206610000),\n FileInfo(path='dbfs:/FileStore/tables/my_sqlite.db', name='my_sqlite.db', size=11264, modificationTime=1679834693000),\n FileInfo(path='dbfs:/FileStore/tables/part_00000_tid_730451297822678341_1dda7027_2071_4d73_a0e2_7fb6a91e1d1f_0_c000.json', name='part_00000_tid_730451297822678341_1dda7027_2071_4d73_a0e2_7fb6a91e1d1f_0_c000.json', size=14831354, modificationTime=1675008606000),\n FileInfo(path='dbfs:/FileStore/tables/part_00001_tid_730451297822678341_1dda7027_2071_4d73_a0e2_7fb6a91e1d1f_0_c000.json', name='part_00001_tid_730451297822678341_1dda7027_2071_4d73_a0e2_7fb6a91e1d1f_0_c000.json', size=14831559, modificationTime=1675008601000),\n FileInfo(path='dbfs:/FileStore/tables/user_artist_data_small.txt', name='user_artist_data_small.txt', size=898234, modificationTime=1678205250000)]"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Parquet\n\nParquet est un format de stockage de données open source orienté colonnes qui offre une variété d'optimisations de stockage, en particulier pour analytics workloads. Il permet la compression en colonnes, ce qui économise de l'espace de stockage et permet de lire des colonnes individuelles plutôt que des fichiers entiers. Nous recommandons d'écrire les données en format Parquet pour un stockage à long terme, car la lecture d'un fichier Parquet sera toujours plus efficace que celle d'un fichier JSON ou CSV.\n\nComme nous venons de le mentionner, il y a très peu d'options Parquet - précisément deux, en fait - parce qu'il a une spécification bien définie qui s'aligne étroitement sur les concepts de Spark.\n\nParquet is an open source column-oriented data store that provides a variety of storage optimizations, especially for analytics workloads. It provides columnar compression, which saves storage space and allows for reading individual columns instead of entire files. We recommend writing data out to Parquet for long-term storage because reading from a Parquet file will always be more efficient than JSON or CSV.\n\nAs we just mentioned, there are very few Parquet options—precisely two, in fact—because it has a well-defined specification that aligns closely with the concepts in Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d0227d9-5c48-4bd2-8ddc-5a1b41747eb4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_parquet = spark.read.format(\"parquet\").load(\"/databricks-datasets/definitive-guide/data/flight-data/parquet/2010-summary.parquet\")\n\ndf_parquet.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4ad06e6-e2f9-4b86-9469-e73a1517c139","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 2. Rows, Columns and Expressions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa2cb745-44bc-4264-bb29-f2c5e82ec6f3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Rows"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b75e1135-ff5f-47e8-a6ae-4a816821df8e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import Row\nmyRow = Row(\"Hello\", None, 1, False)\ntype(myRow)\nmyRow[0]\nmyRow[2]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f05c4fd3-aad2-4dde-b81d-08a05219cf09","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[168]: 1"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Columns\n.col(), .column() or .expr() functions to construct and refer to columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0eaa7c1a-1a14-43ed-a0e5-6f8892a2de54","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, column, expr"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"397b7d46-244d-4bf6-9a10-0d1b923921b7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### select and selectExpr"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82aa0820-b3c4-4e84-b9aa-02c498e0b540","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select('ORIGIN_COUNTRY_NAME').show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8c3f0eb-02ad-4c72-bc9b-457074a0a8f9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------+\n|ORIGIN_COUNTRY_NAME|\n+-------------------+\n|            Romania|\n|            Croatia|\n+-------------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select('ORIGIN_COUNTRY_NAME', 'DEST_COUNTRY_NAME').show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c2a968f-19f8-4143-884a-80d66e7b07e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------+-----------------+\n|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-------------------+-----------------+\n|            Romania|    United States|\n|            Croatia|    United States|\n+-------------------+-----------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\ndf.select(col('DEST_COUNTRY_NAME').alias('destination'), \n          column('DEST_COUNTRY_NAME'), \n          expr('DEST_COUNTRY_NAME as destination'), \n          'DEST_COUNTRY_NAME').show(2)\ndf.selectExpr(\"DEST_COUNTRY_NAME as destination\", \"count\").show(2)\n# we recommend selectExpr for a more flexible syntax\n\n# col(), column() and expr() are generally used within other methodes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e92087c-d0bb-4a7d-aa68-bd47791d5a96","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+-----------------+-------------+-----------------+\n|  destination|DEST_COUNTRY_NAME|  destination|DEST_COUNTRY_NAME|\n+-------------+-----------------+-------------+-----------------+\n|United States|    United States|United States|    United States|\n|United States|    United States|United States|    United States|\n+-------------+-----------------+-------------+-----------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 3. DataFrame Transformations\nSpark DataFrames are inherently unordered and do not support random access. (There is no concept of a built-in index as there is in pandas)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d18a9c5-f4e6-4909-a094-a1b9b8e986a0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 3.1 Creating SQL table (view)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00a21041-c1c2-4ff1-ba2c-d8e4313ee711","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.createGlobalTempView(\"dfTable\") # \"Global\" when you want to share data among different sessions and keep alive until your application ends\n\ndf.createOrReplaceTempView(\"dfTable\") # only within your spark session"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a6f794f-c849-4568-9b87-bfa3c973a20b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Once we register this as a temporary view so that we can query it with SQL\nspark.sql('SELECT * FROM dfTable').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8222a60f-c830-428a-93ff-75c7745a7f92","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|   15|\n|       United States|            Croatia|    1|\n|       United States|            Ireland|  344|\n|               Egypt|      United States|   15|\n|       United States|              India|   62|\n|       United States|          Singapore|    1|\n|       United States|            Grenada|   62|\n|          Costa Rica|      United States|  588|\n|             Senegal|      United States|   40|\n|             Moldova|      United States|    1|\n|       United States|       Sint Maarten|  325|\n|       United States|   Marshall Islands|   39|\n|              Guyana|      United States|   64|\n|               Malta|      United States|    1|\n|            Anguilla|      United States|   41|\n|             Bolivia|      United States|   30|\n|       United States|           Paraguay|    6|\n|             Algeria|      United States|    4|\n|Turks and Caicos ...|      United States|  230|\n|       United States|          Gibraltar|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["%sql select * from dfTable"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cddf2cbe-1e14-45b3-9e98-b5f3fc5e65c7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.2 Add Constant/Literals Column\nSometimes, we need to pass explicit values into Spark that are just a value (rather than a new\ncolumn)\nIn sql, we can write \"select *, 5, \"five\", 5.0 from table\"\n\nBut in spark, we have to convert native types to Spark types with lit() function\n\nPySpark lit() function is used to \n* convert native types to Spark types : This function converts a type in another language to its correspnding Spark representation\n* therefore, it add constant or literal value as a new column to the DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"232de3a7-967c-4e0a-8f92-02f64c2cd3bc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\ndf.select(lit(5), lit(\"five\"), lit(5.0)) \n\n# in SQL\n# SELECT 5, \"five\", 5.0"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db70285a-0ed4-48a4-b346-d36e50d38bc8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# EXO 1\ndf.select(expr(\"*\"), lit(10).alias(\"REF\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1cc503e-8114-46e5-b0db-2a40e43bb0ee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-852666029468171>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"*\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"REF\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-852666029468171>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"*\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"REF\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.3 Adding Columns :  .withColumns('col_name', expression) method"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aafcb850-2d7e-4ad6-b46c-1173606db5c6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.withColumn('REF', lit(5.0)).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01726fc1-730c-4d84-85ec-a180bdea110f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|REF|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|5.0|\n|    United States|            Croatia|    1|5.0|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# EXO 2\ndf.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d79ecf2-3ca5-4df7-94ae-ac4aae7e3ad5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# EXO 3 : Rename (col(), column() or expr())\ndf.withColumn(\"Destination\", col(\"DEST_COUNTRY_NAME\")).columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aaa942de-8b11-4e6a-8d6c-1a50d6b73fe3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[9]: ['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.4 Renaming Columns : .withColumnRenamed()\n\n*Poser question aux stagiaires : pourquoi pas besoin d'appeler col()*\n\nNo need to call col() because no transformation on the column content"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a809e647-a9d8-4f21-ab6b-d06690043e06","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"Destination\").columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98b92603-aa7e-45c7-9a43-2ba2dd50e3b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[12]: ['Destination', 'ORIGIN_COUNTRY_NAME', 'count']"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.5 Removing Columns : .drop()\nRemoving missing value columns in the next notebook"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca1b64fc-33fe-4ebf-bd74-fc8155045736","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.drop(\"ORIGIN_COUNTRY_NAME\", \"count\").columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2a2ba42-b0a8-4347-8cb0-8fadba946f32","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[98]: ['DEST_COUNTRY_NAME']"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.6 Changing a column’s type: cast()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fcd2f631-d82d-4871-9da1-78a167301ac5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.withColumn(\"count\", col(\"count\").cast(\"float\")).show() # we can cast directly a existing column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4e25f15b-4980-43cb-9dae-ef7009f9945c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[101]: ['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count2']"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.7 Filtering Rows\n* .filter() or .where()\n* pyspark or SQL expression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fd5b580b-10bf-4c0d-a2f2-712953f0639c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# The following filters are equivalent\ndf.filter(col(\"count\")<2).show(10)\ndf.where(\"count < 2\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"26b7a49e-fcd1-4c95-b6c7-0ed3fdad3074","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Croatia|    1|\n|       United States|          Singapore|    1|\n|             Moldova|      United States|    1|\n|               Malta|      United States|    1|\n|       United States|          Gibraltar|    1|\n|Saint Vincent and...|      United States|    1|\n|            Suriname|      United States|    1|\n|       United States|             Cyprus|    1|\n|        Burkina Faso|      United States|    1|\n|            Djibouti|      United States|    1|\n+--------------------+-------------------+-----+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["**NOTE** : you might want to put multiple filters into the same expression. Although this is\npossible, it is not always useful, because Spark automatically performs all filtering operations at\nthe same time regardless of the filter ordering (we already mentioned this in **Catalyst Optimiseur** part)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47034119-0253-4ef6-8c3f-fc6608791d73","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# df.filter((col(\"count\")<2) & (col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")).show(10)\n\ndf.filter(col(\"count\")<5).where(col(\"ORIGIN_COUNTRY_NAME\") == \"Croatia\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"faf45c6d-efaa-4d2c-9c8c-d3e639900dfb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.8 Getting Unique Rows:.distinct()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1fca8c00-f98b-4386-a4a8-81988d7f98eb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.count() # 256\n# df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count() # 256\ndf.select(\"ORIGIN_COUNTRY_NAME\").distinct().count() # 125"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4cb40f58-4481-42a1-b709-b307193031dc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[113]: 125"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.9 Random Samples: .sample()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9cf41444-13cc-4611-99b3-adcef0b8be6c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.sample(withReplacement=False, fraction=0.5, seed=42).count()\n\n# prepare the union operation\ndf_test = df.sample(withReplacement=False, fraction=0.5, seed=42)\n\ndf_test = df_test.repartition(5)\ndf_test.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4232cf12-dd0a-44e1-a20d-67a865e4c023","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[114]: 132"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.10 Random Splits : .randomSplit()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"662c210c-8b66-4c4a-ab84-ac205af18a28","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["type(df.randomSplit([0.25, 0.75], seed))\nTest_df, train_df = df.randomSplit([0.25, 0.75], seed)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b73c379-7a84-4c28-977b-89c3db515f0e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[117]: list"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.11 Concatenating and Appending Rows (Union in sql): .union()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f978c3b-d0d6-4707-8ca1-7dea6d859535","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# case 1: union with df\ndf.union(df_test).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a14acd9d-840a-4199-976a-ab37e631907c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2657625770389973>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_test\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'df_test' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df_test' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2657625770389973>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_test\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'df_test' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["# case 2: union with native rows\nschema = df.schema\n\nnewRows = [\n    Row(\"new country 1\", \"other country 1\", 5),\n    Row(\"new country 2\", \"other country 2\", 2)\n]\n\nnewDF = spark.createDataFrame(newRows, schema)\nnewDF.rdd.getNumPartitions()\n\n# no longer need to use the parallelize method on a SparkContext \n# parallelizedRows = spark.sparkContext.parallelize(newRows)\n\n# # To create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession)\n# # type(parallelizedRows) # parallelizedRows is a RDD\n# newDF = spark.createDataFrame(parallelizedRows, schema)\n\n\n(df.union(newDF)\n .filter(\"ORIGIN_COUNTRY_NAME like 'other country%'\")\n .show(5))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6da64c89-0da6-41c3-b33d-47621bedbe37","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.12 Sorting Rows: .sort() or .orderBy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"238b6a25-910c-40ac-97e2-9a013742d160","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import asc, desc\ndf.sort(desc(\"count\"), \"DEST_COUNTRY_NAME\").show(5)  # no need to use col() function "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f48749af-00a6-4a27-9a7d-cf4352881293","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|        The Bahamas|  986|\n|      The Bahamas|      United States|  955|\n|    United States|             France|  952|\n|           France|      United States|  935|\n|    United States|              China|  920|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c90ccb1a-f91d-4148-83a7-2f36e1a28a86","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+--------------------+-----+\n|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n+-----------------+--------------------+-----+\n|    United States|         The Bahamas|  986|\n|      The Bahamas|       United States|  955|\n|    United States|              France|  952|\n|           France|       United States|  935|\n|    United States|               China|  920|\n|          Curacao|       United States|   90|\n|         Colombia|       United States|  873|\n|    United States|            Colombia|  867|\n|           Brazil|       United States|  853|\n|    United States|              Canada| 8483|\n|           Canada|       United States| 8399|\n|     Saudi Arabia|       United States|   83|\n|    United States|             Curacao|   83|\n|    United States|         South Korea|  827|\n|    United States|British Virgin Is...|   80|\n|      Netherlands|       United States|  776|\n|            China|       United States|  772|\n|    United States|         New Zealand|   74|\n|    United States|              Mexico| 7187|\n|           Mexico|       United States| 7140|\n+-----------------+--------------------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["For optimization purposes, it’s sometimes advisable to sort within each partition before another\nset of transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73404175-6346-4b3c-b923-0a6fb2d58694","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = df.repartition(5)\ndf.rdd.getNumPartitions()\n\ndf.sortWithinPartitions(desc(\"count\")) # 0.12 seconds\n\n# df.sort(desc(\"count\")) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3518354d-9078-4fb8-905d-9955ae38def2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[19]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: string]"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3.13 .repartition() and .coalesce()\n* .repartition() : If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column\n\n* .Coalesce(): will not incur a full shuffle and will try to combine partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a97c9d70-98bb-4f2d-9021-2943de0e40f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.rdd.getNumPartitions()\ndf= df.repartition(5) # nativement spark will do a fair partitions "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01feb4de-b9bd-4f96-96f1-447d9150daa9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import spark_partition_id, asc, desc\ndf\\\n    .withColumn(\"partitionId\", spark_partition_id())\\\n    .groupBy(\"partitionId\")\\\n    .count()\\\n    .orderBy(asc(\"count\"))\\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db1fc4be-dcca-440c-9f4b-0f97dfc0ff9a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------+-----+\n|partitionId|count|\n+-----------+-----+\n|          1|   33|\n|          0|   43|\n|          2|   45|\n|          3|   52|\n|          4|   83|\n+-----------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df= df.repartition(5, col(\"count\")) # This operation will shuffle your data into five partitions based on the destination country name\n# If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column\n# we can also use col(\"count\"), but col() is not necessary\n\n# [re-exécuter la commande au-dessus]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6ba676b-4c1f-4c1c-b51c-d702c893713c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = df.coalesce(2) # will not incur a full shuffle and will try to combine partitions. \ndf.rdd.getNumPartitions() # 2\n\n# the re-execute the previous count command"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d63f48f-82bc-4177-9a70-be15f9138f7e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[36]: 2"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CEPE_01_Big_data_processing_1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1370070581318520}},"nbformat":4,"nbformat_minor":0}
