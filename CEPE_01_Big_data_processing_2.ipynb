{"cells":[{"cell_type":"markdown","source":["### 4. Working with Different Types of Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0098d356-3fc8-421a-bf14-7c017129ed5f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = (spark.read.format(\"csv\")\n .option(\"header\", \"true\")\n .option(\"inferSchema\", \"true\")\n .load(\"/databricks-datasets/definitive-guide/data/retail-data/by-day/2010-12-01.csv\")\n)\ndf.printSchema()\ndf.count()\ndf.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63dd9205-3d8d-40c2-95ec-b3766eb2e7cc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["**NOTE About inferSchema**\n\nUsing inferSchema=false (default option) will give a dataframe where all columns are strings (StringType).\n\nBy setting inferSchema=true, Spark will automatically go through the csv file and infer the schema of each column. This requires an extra pass over the file which will result in reading a file with inferSchema set to true being slower. But in return the dataframe will most likely have a correct schema given its input.\n\nUsing inferSchema=True, there is no longer a lazy evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2989f5e5-6a89-41e4-885a-c01f273190b8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 4.1 Converting to Spark Types (reminder)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e012eef7-a6e9-47ab-a94e-198e96b06f2b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\ndf.select(\"InvoiceNo\", lit(5), lit(\"five\"), lit(5.0))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f985cba-7b6d-47c5-bc1a-1a8b0ebb6634","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[30]: DataFrame[InvoiceNo: string, 5: int, five: string, 5.0: double]"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 4.2 Working with Booleans"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a543fe9-f7aa-428a-83e5-829f15b2fbfd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf.filter(col(\"InvoiceNo\") == \"536365\").show() # same as .where()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"634df2dc-e2d9-45ee-80d7-2f851d6fdb24","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+-----------------------------------+\n|InvoiceNo|Description                        |\n+---------+-----------------------------------+\n|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n|536365   |WHITE METAL LANTERN                |\n|536365   |CREAM CUPID HEARTS COAT HANGER     |\n|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n+---------+-----------------------------------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.where(\"InvoiceNo <> '536365'\").show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4026ceb7-21cd-48d9-8441-a87fc8ff6663","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# instr() # position of work, begins from 1\nfrom pyspark.sql.functions import instr\n(df.withColumn(\"isWhite\", instr(df.Description, \"WHITE\"))\n    .select('Description', 'isWhite')\n    .show(5, False)\n)\n\n# .isin()\ndf.where(df.StockCode.isin(\"DOT\", \"22633\")).select(\"StockCode\", \"Country\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a5ebfc5-dc56-4b53-afa3-25d161d49f85","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------------+\n|StockCode|       Country|\n+---------+--------------+\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|    22633|United Kingdom|\n|      DOT|United Kingdom|\n|    22633|United Kingdom|\n|      DOT|United Kingdom|\n+---------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7114c3bf-a8a7-4c8e-8f45-6a46c34065bf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[58]: [Row(s=2)]"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 4.3 Working with Numbers"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a50702d0-2c44-4285-97c5-f6fcb12854ca","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# By default, the round function rounds up if youâ€™re exactly in between two numbers. You can round down by using the bround\n\n# from pyspark.sql.functions import lit, round, bround\n\n\n# df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(20)\n\n\n\n# df.stat.freqItems([\"Country\"]).show(1, False)\n\n# df.select(\"Country\").distinct().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f4a08d7c-04c3-42c6-a500-de45b8d281bd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[["90214E","20728","20755","21703","22113","22524","22041","72803A","72798C","90181B","21756","22694","90206C","20970","21624","90209C","84744","82494L","22952","20682","22583","21705","20679","22220","90177E","90214A","22448","90214S","22121","22802","84970L","72818","90192","90200C","22910","21380","90211A","21137","35271S","84926A","20765","22384","21524","22165","22366","21221","21704","22519","85035C","21967","22114","22909","22900","22447","21577","21877","20726","85034A","DOT","84658","21472","22804","22222","72802C","21739","22467","90214H","22785","22446","22197","20665","21733","22731","21709","22086","40001","85123A"],[200,128,23,32,50,600,8,17,80,-1,-10,11,56,47,20,-7,2,5,480,-4,14,432,100,64,40,13,4,-5,22,16,-2,7,70,384,25,34,10,1,288,216,28,252,19,120,192,60,96,72,144,36,27,9,18,48,21,12,3,-6,-24,30,15,33,6,24,-12,-3]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"StockCode_freqItems","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"Quantity_freqItems","type":"{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>StockCode_freqItems</th><th>Quantity_freqItems</th></tr></thead><tbody><tr><td>List(90214E, 20728, 20755, 21703, 22113, 22524, 22041, 72803A, 72798C, 90181B, 21756, 22694, 90206C, 20970, 21624, 90209C, 84744, 82494L, 22952, 20682, 22583, 21705, 20679, 22220, 90177E, 90214A, 22448, 90214S, 22121, 22802, 84970L, 72818, 90192, 90200C, 22910, 21380, 90211A, 21137, 35271S, 84926A, 20765, 22384, 21524, 22165, 22366, 21221, 21704, 22519, 85035C, 21967, 22114, 22909, 22900, 22447, 21577, 21877, 20726, 85034A, DOT, 84658, 21472, 22804, 22222, 72802C, 21739, 22467, 90214H, 22785, 22446, 22197, 20665, 21733, 22731, 21709, 22086, 40001, 85123A)</td><td>List(200, 128, 23, 32, 50, 600, 8, 17, 80, -1, -10, 11, 56, 47, 20, -7, 2, 5, 480, -4, 14, 432, 100, 64, 40, 13, 4, -5, 22, 16, -2, 7, 70, 384, 25, 34, 10, 1, 288, 216, 28, 252, 19, 120, 192, 60, 96, 72, 144, 36, 27, 9, 18, 48, 21, 12, 3, -6, -24, 30, 15, 33, 6, 24, -12, -3)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Convertir vers \ndf.stat.crosstab(\"Country\", \"StockCode\").display()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98e4bf64-64c7-4836-a91f-9edd8b5d4d45","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<bound method PandasConversionMixin.toPandas of DataFrame[StockCode_Quantity: string, -1: bigint, -10: bigint, -12: bigint, -2: bigint, -24: bigint, -3: bigint, -4: bigint, -5: bigint, -6: bigint, -7: bigint, 1: bigint, 10: bigint, 100: bigint, 11: bigint, 12: bigint, 120: bigint, 128: bigint, 13: bigint, 14: bigint, 144: bigint, 15: bigint, 16: bigint, 17: bigint, 18: bigint, 19: bigint, 192: bigint, 2: bigint, 20: bigint, 200: bigint, 21: bigint, 216: bigint, 22: bigint, 23: bigint, 24: bigint, 25: bigint, 252: bigint, 27: bigint, 28: bigint, 288: bigint, 3: bigint, 30: bigint, 32: bigint, 33: bigint, 34: bigint, 36: bigint, 384: bigint, 4: bigint, 40: bigint, 432: bigint, 47: bigint, 48: bigint, 480: bigint, 5: bigint, 50: bigint, 56: bigint, 6: bigint, 60: bigint, 600: bigint, 64: bigint, 7: bigint, 70: bigint, 72: bigint, 8: bigint, 80: bigint, 9: bigint, 96: bigint]>\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id, expr\ndf.select(monotonically_increasing_id().alias(\"id\"), expr(\"*\")).show(20)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c40a6ed7-c5bb-4575-9095-b18321a29e6e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------------------+\n|monotonically_increasing_id()|\n+-----------------------------+\n|                            0|\n|                            1|\n|                            2|\n|                            3|\n|                            4|\n|                            5|\n|                            6|\n|                            7|\n|                            8|\n|                            9|\n|                           10|\n|                           11|\n|                           12|\n|                           13|\n|                           14|\n|                           15|\n|                           16|\n|                           17|\n|                           18|\n|                           19|\n+-----------------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 4.4 Working withStrings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1c0b1ab-f824-4909-85eb-d4754b177eab","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# initcap, lower, upper\nfrom pyspark.sql.functions import initcap, lower, upper\n\n\n(df.select(col(\"Description\"),\n           lower(col(\"Description\")),\n           upper(lower(col(\"Description\"))),\n           initcap(col(\"Description\"))\n          ).show(2))\n\n\n(df.select(col(\"Description\"),\n           lower((\"Description\")),\n           upper(\"Description\"),\n           initcap(col(\"Description\"))\n          ).show(2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a42dbc8-dc42-4ab5-98ef-82b7ef3f6cfd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+--------------------+--------------------+--------------------+\n|         Description|  lower(Description)|  upper(Description)|initcap(Description)|\n+--------------------+--------------------+--------------------+--------------------+\n|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|White Hanging Hea...|\n| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN| White Metal Lantern|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# ltrim, rtrim, rpad, lpad, trim, substring\nfrom pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim, substring\n(df.select(\n    ltrim(lit('   HELLO  ')).alias('ltrim'),\n    rtrim(lit('   HELLO  ')).alias('rtrim'),\n    substring(lit('HELLO'), 1, 3).alias('substr'),\n    rpad(lit('HELLO'), 10, \"x\").alias('rap'),\n    lpad(lit('HELLO'), 10, \"x\").alias('lpad')\n\n).show(1))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0683ca1c-00fe-41e8-9827-9553ead06623","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2413851590457828>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mltrim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrtrim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrpad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlpad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubstring\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m (df.select(\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mltrim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'   HELLO  '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'ltrim'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mrtrim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'   HELLO  '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'rtrim'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0msubstring\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'HELLO'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'substr'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2413851590457828>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mltrim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrtrim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrpad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlpad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubstring\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m (df.select(\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mltrim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'   HELLO  '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'ltrim'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mrtrim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'   HELLO  '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'rtrim'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0msubstring\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'HELLO'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'substr'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Regular Expressions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a05268a-be66-4c73-892a-e74477684d2f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\nregex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\ndf.select(\nregexp_replace(\"Description\", regex_string, \"COLOR\").alias(\"color_clean\"),\n\"Description\").show(2)\n\n\nfrom pyspark.sql.functions import regexp_extract\nextract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\ndf.select(\nregexp_extract(\"Description\", extract_str, 0).alias(\"color_clean\"),\n\"Description\").show(2)\n\n# idx indicates which regex group to extract. An idx of 0 means matching the entire regular expression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dbca3a58-0b1f-4c2e-8fec-0df3c411db09","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+--------------------+\n|         color_clean|         Description|\n+--------------------+--------------------+\n|COLOR HANGING HEA...|WHITE HANGING HEA...|\n| COLOR METAL LANTERN| WHITE METAL LANTERN|\n+--------------------+--------------------+\nonly showing top 2 rows\n\n+-----------+--------------------+\n|color_clean|         Description|\n+-----------+--------------------+\n|      WHITE|WHITE HANGING HEA...|\n|      WHITE| WHITE METAL LANTERN|\n+-----------+--------------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr, locate\nsimpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\ndef color_locator(column, color_string):\n    return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\nselectedColumns.append(expr(\"*\")) # has to a be Column type\n\ndf.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n.show(3, False)\n\n\nselectedColumns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7c59c34-e26f-4465-bca1-930b4053fa5a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------+--------+------+--------+-------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|is_black|is_white|is_red|is_green|is_blue|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+--------+--------+------+--------+-------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|false   |true    |false |false   |false  |536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n|false   |true    |false |false   |false  |536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n|false   |true    |true  |false   |false  |536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.    |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n+--------+--------+------+--------+-------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 4.5 Working with Dates and Timestamps"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a57f66a-f2fd-414e-ba4f-e29c762e2fbd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import current_date, current_timestamp\ndateDF = spark.range(1).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())\ndateDF.show(1, False)\ndateDF.createOrReplaceTempView(\"dateTable\")\ndateDF.printSchema()\n\n\nfrom pyspark.sql.functions import date_add, date_sub\ndateDF.select(date_sub(\"today\", 5), date_add(\"today\", 5)).show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5550e6cc-6837-4335-b612-40b0415180f1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+-----------------------+\n|id |today     |now                    |\n+---+----------+-----------------------+\n|0  |2023-04-05|2023-04-05 16:55:28.232|\n+---+----------+-----------------------+\n\nroot\n |-- id: long (nullable = false)\n |-- today: date (nullable = false)\n |-- now: timestamp (nullable = false)\n\n+------------------+------------------+\n|date_sub(today, 5)|date_add(today, 5)|\n+------------------+------------------+\n|        2023-03-31|        2023-04-10|\n+------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, to_timestamp, datediff, months_between, year, month\n(df.withColumn('date', to_date('InvoiceDate'))\n .withColumn('year', year('InvoiceDate'))\n .show(2))\n\n# EXO : une date ref, calculer monthe between\n\n# (df.withColumn('date', to_date('InvoiceDate')).select(\"date\").distinct().show()) # il y a une seule date dans ce df\n\ndf.withColumn('date_ref', lit('2013-10-01')).withColumn('delai', months_between('date_ref', 'InvoiceDate')).show() # in months_between function no need for col()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8772dded-33bc-402f-8c0f-b88a4c8d5b22","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------+-----+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|  date_ref|delai|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------+-----+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|2013-10-01| 34.0|\n|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|2013-10-01| 34.0|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 4.6 Working with Nulls in Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9e17822-fb5a-499d-b62f-4b28f38a4e1a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.printSchema\ndf.filter(\"Description is NULL\").show() # InvoiceNo = 536414\ndf.filter(col(\"Description\").isNull()).show() # here, col() is needed \n\n# or we can do \n# df.filter(df.description.isNull()).show()\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c43f5390-7076-4363-985f-50608c0c1ae4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1445277380721950>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Description is NULL\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# InvoiceNo = 536414\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Description\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misNull\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# here, col() is needed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# or we can do\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1445277380721950>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Description is NULL\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# InvoiceNo = 536414\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Description\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misNull\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# here, col() is needed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# or we can do\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["# coalesce()\n# select the first non-null value from a set of columns by using the coalesce function.\n\nfrom pyspark.sql.functions import coalesce\ndf.filter(\"Description is NULL\").show()\n\ndf.filter(\"CustomerID is NULL\").select(\"Description\", \"Country\",coalesce(\"Description\", \"Country\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f21d7411-9eb6-4146-aa3e-2b700032228a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------------------------+\n|coalesce(Description, CustomerId)  |\n+-----------------------------------+\n|WHITE HANGING HEART T-LIGHT HOLDER |\n|WHITE METAL LANTERN                |\n|CREAM CUPID HEARTS COAT HANGER     |\n|KNITTED UNION FLAG HOT WATER BOTTLE|\n|RED WOOLLY HOTTIE WHITE HEART.     |\n|SET 7 BABUSHKA NESTING BOXES       |\n|GLASS STAR FROSTED T-LIGHT HOLDER  |\n|HAND WARMER UNION JACK             |\n|HAND WARMER RED POLKA DOT          |\n|ASSORTED COLOUR BIRD ORNAMENT      |\n|POPPY'S PLAYHOUSE BEDROOM          |\n|POPPY'S PLAYHOUSE KITCHEN          |\n|FELTCRAFT PRINCESS CHARLOTTE DOLL  |\n|IVORY KNITTED MUG COSY             |\n|BOX OF 6 ASSORTED COLOUR TEASPOONS |\n|BOX OF VINTAGE JIGSAW BLOCKS       |\n|BOX OF VINTAGE ALPHABET BLOCKS     |\n|HOME BUILDING BLOCK WORD           |\n|LOVE BUILDING BLOCK WORD           |\n|RECIPE BOX WITH METAL HEART        |\n+-----------------------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# .drop()\n# The simplest function is drop, which removes rows that contain nulls.\ndf.count() # 3108\ndf.na.drop().count() #1968\ndf.na.drop(\"any\").count() # as same as the previous row\ndf.na.drop(\"all\").count() # 3108 all column null\n\ndf.na.drop(\"any\", subset=[\"StockCode\", \"description\"]).count() # 3098"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43f937fe-25a5-4c88-a3c4-72b42740f2e6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[8]: 3098"]}],"execution_count":0},{"cell_type":"code","source":["# .fill()\n\n\ndf.na.fill(5, subset=[\"CustomerID\", \"Description\"]).filter(col(\"InvoiceNo\")==536414).show()\n# pay attention to result, because of data type, descripton is still null\n\n\n# so the solution is\nfill_cols_vals = {\"CustomerID\": 5, \"Description\" : \"No Value\"}\ndf.na.fill(fill_cols_vals).filter(col(\"InvoiceNo\")==536414).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"776fc951-44ec-4789-9bca-1da1aab4538b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|       5.0|United Kingdom|\n+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# .replace() : this is not about missin value but a more general replacement\ndf.printSchema()\ndf.filter(col(\"Description\")==\"\").show()\nfrom pyspark.sql.functions import col\n\ndf.replace([\"United Kingdom\"], [\"UNKNOWN\"], \"Country\").filter(col(\"InvoiceNo\")==536414).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe06ff4f-b87a-45bd-93b4-6dd3e3d729a3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+-----------+--------+-------------------+---------+----------+-------+\n|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|Country|\n+---------+---------+-----------+--------+-------------------+---------+----------+-------+\n|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|      null|UNKNOWN|\n+---------+---------+-----------+--------+-------------------+---------+----------+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 4.7 Working with Complex Types\n\n* Le monde ne tourne pas qu'au tour de tabular structurÃ©\n* Intresting when you do a lot webscrapping and json manipulaton"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"495b9d72-8770-431e-800c-12d789bf004a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# structs() creates array within DataFrames\n# df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show()\n\nfrom pyspark.sql.functions import struct\n\ncomplexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n\ncomplexDF.show(5, False)\n\ncomplexDF.select(col(\"complex\").getField(\"Description\")).show()\n\n# if in your data there is alreay a complexe type that you want to remodeling\ncomplexDF.select(\"complex.*\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fc86c6a-a6be-43cc-b3ee-ab591800f3c3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------------------------------------------+\n|complex                                      |\n+---------------------------------------------+\n|{WHITE HANGING HEART T-LIGHT HOLDER, 536365} |\n|{WHITE METAL LANTERN, 536365}                |\n|{CREAM CUPID HEARTS COAT HANGER, 536365}     |\n|{KNITTED UNION FLAG HOT WATER BOTTLE, 536365}|\n|{RED WOOLLY HOTTIE WHITE HEART., 536365}     |\n+---------------------------------------------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["**Arrays** List ????\n\nTo define arrays, letâ€™s work through a use case. With our current data, our objective is to take every single word in our Description column and convert that into a row in our DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddd09d2d-d774-48a9-856f-3699d3828cc5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import split\ndf.select(col(\"Description\")).show(2)\n\ndf.select(split(\"Description\", \" \")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8bd4bd1-bfeb-4301-8a3d-a989433fa7ba","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|         Description|\n+--------------------+\n|WHITE HANGING HEA...|\n| WHITE METAL LANTERN|\n+--------------------+\nonly showing top 2 rows\n\n+-------------------------+\n|split(Description,  , -1)|\n+-------------------------+\n|     [WHITE, HANGING, ...|\n|     [WHITE, METAL, LA...|\n+-------------------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import split, explode\n\ndf.select(split(\"Description\", \" \")).show(2, False)\n\n\n# Opposite of explode collect_set (or collect_list)\n\ndf.select(\"Description\", \"InvoiceNo\",explode(split(\"Description\", \" \"))).show(6, False)\n# .count() # 14414 dupliquer les lignes\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4d266553-1113-4fab-86e1-6cecb184d68d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[40]: 14414"]}],"execution_count":0},{"cell_type":"code","source":["# create_map : key -> value\nfrom pyspark.sql.functions import create_map\n\ndf.select(create_map(\"InvoiceNo\", \"Description\").alias(\"complex_map\")).show(5, False)\n\ndf.select(create_map(\"InvoiceNo\", \"Description\").alias(\"complex_map\")).selectExpr(\"complex_map[536365]\").show(5, False)\n\n# you can also explode map type, which will turn them into columns\n# df.select(create_map(\"InvoiceNo\", \"Description\").alias(\"complex_map\")).withColumn(\"Explode\", explode(col(\"complex_map\"))).show(5, False) # this does not work, try to find why\n\ndf.select(create_map(\"InvoiceNo\", \"Description\").alias(\"complex_map\")).selectExpr(\"explode(complex_map)\").show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6a4cd01-dd24-455f-90f6-af7135c55cfe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+------+\n|                 key| value|\n+--------------------+------+\n|WHITE HANGING HEA...|536365|\n| WHITE METAL LANTERN|536365|\n+--------------------+------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# array_contains\n# intresting for complex join\n\nfrom pyspark.sql.functions import array_contains\ndf.select(\"Description\",\n          split(\"Description\", \" \").alias(\"split\"),\n          array_contains(split(\"Description\", \" \"), \"WHITE\").alias('white')).show(2, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37086618-4071-4ffe-b982-4166eed795f6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Working with JSON"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b45f9b72-ba72-404e-a8a7-8bf1c2d5f24e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import from_json, to_json\nfrom pyspark.sql.types import *\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n.select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n.select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20d4d44f-0b6c-4ab0-bc2f-81550a9ffb57","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1445277380721957>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselectExpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"(InvoiceNo, Description) as myStruct\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mto_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"myStruct\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"newJSON\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfrom_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"newJSON\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"newJSON\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: from_json() missing 1 required positional argument: 'schema'","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: from_json() missing 1 required positional argument: 'schema'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1445277380721957>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselectExpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"(InvoiceNo, Description) as myStruct\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mto_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"myStruct\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"newJSON\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfrom_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"newJSON\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"newJSON\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: from_json() missing 1 required positional argument: 'schema'"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### User-Defined Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"acff95ff-ffb9-4930-91e4-ef898fc9f99b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# function python\ndef power3(double_value):\n    return double_value ** 3\npower3(2.0)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2873fc2-d25a-4180-8ff6-6086d9f0deb7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[77]: 8.0"]}],"execution_count":0},{"cell_type":"markdown","source":["Now that weâ€™ve created these functions and tested them, we need to register them with Spark so that we can use them on all of our worker machines. Spark will serialize the function on the driver and transfer it over the network to all executor processes. This happens regardless of language. When you use the function, there are essentially two different things that occur. If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you canâ€™t take advantage of code generation capabilities that Spark has for built-in functions.\n\nIf the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark.\n\n**why pyspark functions works with performance but not Python UDF function?**\n\nhttps://stackoverflow.com/questions/38296609/spark-functions-vs-udf-performance\n\nSpark DataFrame is natively a JVM structure and standard access methods are implemented by simple calls to Java API. UDF from the other hand are implemented in Python and require moving data back and forth.\n\nWhile PySpark in general requires data movements between JVM and Python, in case of low level RDD API it typically doesn't require expensive serde activity. Spark SQL adds additional cost of serialization and serialization as well cost of moving data from and to unsafe representation on JVM. The later one is specific to all UDFs (Python, Scala and Java) but the former one is specific to non-native languages.\n\nUnlike UDFs, **Spark SQL functions** such as pyspark function operate directly on JVM and typically are well integrated with both Catalyst and Tungsten. It means these can be optimized in the execution plan and most of the time can benefit from codgen and other Tungsten optimizations. Moreover these can operate on data in its \"native\" representation."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8086523-62cd-4370-9762-209c9d90b8ce","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# in Python\n\n# First, we need to register the function with Spark to make it available as a DataFrame function, so that we can use them on all of our executor processes machines.Spark will serialize the function on the driver and transfer it over the network to all executor processes with udf().\nfrom pyspark.sql.functions import udf\npower3udf= udf(power3)\n\n\nExampleDF = spark.range(5).withColumnRenamed('id', 'num')\nExampleDF.show()\n\nExampleDF.select(power3udf(\"num\"), \"num\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4aa81eb-4650-4e3a-a4c0-273c9109c928","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------+\n|power3(num)|\n+-----------+\n|          0|\n|          1|\n|          8|\n|         27|\n|         64|\n+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["*Comment about import elements*\n### 5. Aggregations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e398d596-7639-41f9-847e-63440c97909d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_Schema = df.schema\n\ndf = (spark.read.format(\"csv\")\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\") # avoir to use this, prefer lazy evaluation\n#       .schema(df_Schema) # tu ne peux pas utiliser cela directement car dans les data sur la colonne date il y a un prob, il arrive pas a assiger type timestamps\n      .load(\"/databricks-datasets/definitive-guide/data/retail-data/all/*.csv\")\n      .where(\"InvoiceDate like '%2010%'\") # 42481\n#       .where(\"CustomerId IS NOT NULL\")\n      .filter(col(\"InvoiceDate\").like(\"%2010%\")) # 42481\n      .coalesce(5))\n      \n\ndf.rdd.getNumPartitions() # 8 by default and 5 after .coalesce(5)\ndf.cache()\ndf.createOrReplaceTempView(\"dfTable\")\n\ndf.show(5)\ndf.schema\n\ndf.count() #541909 => 42481 after filter\n# df.where(\"CustomerId IS NOT NULL\").count() # 406829"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa3c8219-341e-4f83-a3ca-b8960e264670","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[46]: 0"]}],"execution_count":0},{"cell_type":"markdown","source":["**NOTE : Spark â€“ Difference between Cache and Persist**\n\nSpark Cache and persist are optimization techniques for iterative and interactive Spark applications to improve the performance of the jobs or applications.\n\nhttps://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c8eca2c2-281a-4805-b4c4-e87cf6dcfa2b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 5.1 Aggregation Functions\n* combines with select() function but not selectExpr()\n* df.selectExpr(\"stockCode\").show(5) ==> OK\n* df.selectExpr(countDistinct(\"stockCode\")).show(5) ==> NOT OK, use select()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e95e0a8d-f78b-4efe-a665-4e46e346f771","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# count : The first function worth going over is count, except in this example it will perform as a transformation instead of an action\nfrom pyspark.sql.functions import count\ndf.select(count(\"stockCode\")).show() # je Ã©cris, vous commentez. ici count action or transformation. count() is a function here\n\ndf.select(count(\"*\")).show() # 406829"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37e9cd68-abbb-4f71-b991-77bf6959ad1d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# countDistinct\nfrom pyspark.sql.functions import countDistinct\ndf.select(countDistinct(\"stockCode\")).show() # 4070"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68e6c7e2-db41-494e-8441-4dde539c85fc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# approx_count_distinct\n# Often, we find ourselves working with large datasets and the exact distinct count is irrelevant. There are times when an approximation to a certain degree of accuracy will work just fine\n\nfrom pyspark.sql.functions import approx_count_distinct\ndf.select(approx_count_distinct(\"stockCode\", 0.05)).show() # maximum relative standard deviation allowed (default = 0.05)\n\n# approx_count_distinct took another parameter with which you can specify the maximum estimation error allowed. In this case, we specified a rather large error andthus receive an answer that is quite far off but does complete more quickly than countDistinct. You will see much greater performance gains with larger datasets."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e3ae86df-93d1-4abd-bfe1-3a57535edc7e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------------------+\n|approx_count_distinct(StockCode)|\n+--------------------------------+\n|                            3804|\n+--------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# first and last\nfrom pyspark.sql.functions import first, last\ndf.select(first(\"StockCode\"), last(\"StockCode\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ebba7e8-1739-4356-835c-a4610a0dd75b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# min and max\n\nfrom pyspark.sql.functions import min, max\ndf.select(min(\"Quantity\"), max(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32207b9a-ed06-4b99-a6d0-9e8a83605edc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# sum\nfrom pyspark.sql.functions import sum\ndf.select(sum(\"Quantity\")).show() # 342228\n\nspark.sql(\"SELECT sum(Quantity) FROM dfTable\").show() # 342228"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab04ecca-094c-4c74-96fa-4ee427aa1f17","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# sumDistinct\nfrom pyspark.sql.functions import sumDistinct\ndf.select(sumDistinct(\"Quantity\")).show() # 22884"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7964ed26-0592-4694-858f-4309e0c99514","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# avg\nfrom pyspark.sql.functions import avg, sum\ndf.show(5)\ndf.select(avg(\"Quantity\")).show()\n\n# EXO, calculer montant moyen par trasaction\nfrom pyspark.sql.functions import expr\n(\n    df.select(\n    sum(expr(\"Quantity * UnitPrice\")).alias(\"Price\"), \n    count(\"Quantity\").alias(\"Total_Transactions\"))\n .selectExpr(\"Price/Total_Transactions as avg_price\").show(4)\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"55c87c5a-f2b0-4137-b28a-7a2276c8cdcf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Variance and Standard Deviation\nfrom pyspark.sql.functions import var_pop, stddev_pop, var_samp, stddev_samp\n(df.select(var_pop(\"Quantity\"), \n          var_samp(\"Quantity\"),\n         stddev_pop(\"Quantity\"),\n         stddev_samp(\"Quantity\")).show())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1a51c4c-651a-4917-bb5c-721b2fedf8ed","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# skewness and kurtosis\n# Both measurements of extreme points in your data. Skewness measures the asymmetry of the values in your data around the mean, whereas kurtosis is a measure of the tail of data. These are both relevant specifically when modeling your data as a probability distribution of a random variable\nfrom pyspark.sql.functions import skewness, kurtosis\ndf.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02ba5fc2-8cb6-48a3-a85a-38358e4ce384","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------+------------------+\n| skewness(Quantity)|kurtosis(Quantity)|\n+-------------------+------------------+\n|-0.2640755761052562|119768.05495536952|\n+-------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Covariance and Correlation\n# corr : correlation\n# covar_pop : the population covariance\n# covar_samp : sample covariance\nfrom pyspark.sql.functions import corr, covar_pop, covar_samp\ndf.select(corr(\"InvoiceNo\",\"Quantity\"), covar_pop(\"InvoiceNo\",\"Quantity\"), covar_samp(\"InvoiceNo\",\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1411107-3eb0-4aa0-98b9-9317d54ae585","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------------+------------------------------+-------------------------------+\n|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n+-------------------------+------------------------------+-------------------------------+\n|     4.912186085635685E-4|            1052.7260778741693|             1052.7280543902734|\n+-------------------------+------------------------------+-------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Aggregating to Complex Types, opposite is explode()\n\n# The collect_list() operation is not responsible for unifying the array list. It fills all the elements by their existing order and does not remove any of the duplications. On the other hand, the collect_set() operation does eliminate the duplicates; however, it cannot save the existing order of the items in the array.\n\nfrom pyspark.sql.functions import collect_list, collect_set\n\ndf.select(collect_list(\"Country\"), collect_set(\"Country\")).show() # we can also use select()\n# df.agg(collect_list(\"Country\"), collect_set(\"Country\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2326b43-b6f4-4739-ad6a-7a9f40a51b64","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------------------+--------------------+\n|collect_list(Country)|collect_set(Country)|\n+---------------------+--------------------+\n| [United Kingdom, ...|[Portugal, Italy,...|\n+---------------------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 5.2 Grouping\n\ndf.grouBy.unique_aggregation_function()\n\ndf.grouBy.agg(function_1, function_2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83092c54-8465-4014-964c-8a08ca8c7e1e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# POSER LA QUESTION : count here is a transformation or a action\ndf.groupBy(\"Country\", \"\").count().show(5) # note that count() is a transformation not a action"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f6b3d2b-7d4e-42ec-80a0-7fdb60f86346","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1077835294916368>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Country\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Quantity\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1077835294916368>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Country\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Quantity\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["# Grouping with Maps\ndf.groupBy(\"Country\").agg(sum(\"Quantity\"), \n                          count(\"Quantity\")).show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f286644-d6a2-47fe-826d-75c894431a03","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------------+-------------+\n|  Country|count(Quantity)|sum(Quantity)|\n+---------+---------------+-------------+\n|   Sweden|            462|        35637|\n|Singapore|            229|         5234|\n|  Germany|           9495|       117448|\n|   France|           8557|       110480|\n|   Greece|            146|         1556|\n+---------+---------------+-------------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\ndf.groupBy(\"Country\").agg(avg(\"Quantity\"), stddev_pop(\"Quantity\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb28ab89-1204-4d4d-95a2-e4fd9383cc2b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+-----------------+--------------------+\n|  Country|    avg(Quantity)|stddev_pop(Quantity)|\n+---------+-----------------+--------------------+\n|   Sweden|77.13636363636364|  128.75197143945962|\n|Singapore|22.85589519650655|  27.682246020195702|\n+---------+-----------------+--------------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 5.3 Window Functions\n##### Example 1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"230e3865-979d-4479-a986-55fb8c241529","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Copier coller les code depuis codeshare\nfrom pyspark.sql import Row\nsimpleData = [Row(\"James\", \"Sales\", 3000),\n    Row(\"Michael\", \"Sales\", 4600),\n    Row(\"Robert\", \"Sales\", 4100),\n    Row(\"Maria\", \"Finance\", 3000),\n    Row(\"James\", \"Sales\", 3000),\n    Row(\"Scott\", \"Finance\", 3300),\n    Row(\"Jen\", \"Finance\", 3900),\n    Row(\"Jeff\", \"Marketing\", 3000),\n    Row(\"Kumar\", \"Marketing\", 2000),\n    Row(\"Saif\", \"Sales\", 4100)]\n\n# attention a toDF pour renommer\ndata = spark.createDataFrame(simpleData).toDF(\"employee_name\", \"department\", \"salary\")\n\ndata.printSchema()\ndata.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83ac1c55-e5c0-45ef-9ae7-c6efb819aa43","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["%md\n* The first step to a window function is to create a window specification, with *partitionby* (is unrelated to the partitioning scheme concept)\n* The ordering determines the ordering within a given partition. The frame specification (the rowsBetween statement) states which rows will be included in the frame based on its reference to the current input row."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce1e1553-8274-4572-a606-9e2e381baa8a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.window import Window\nwindowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n\ntype(windowSpec)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a1ffcd6-123a-45bb-bd0a-2e0cf81aafa2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: pyspark.sql.window.WindowSpec"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import row_number # syntaxe : function().over(partition)\ndata.withColumn(\"row_number\",row_number().over(windowSpec)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ede9505d-aa7b-4d8f-9ceb-6ee3a68a1e4c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import rank, dense_rank, cume_dist, lag, lead, max, min\n# pay attention, you must import max and min\n\n(data\n .withColumn(\"rank\", rank().over(windowSpec))\n .withColumn(\"dense_rank\", dense_rank().over(windowSpec))\n .withColumn(\"cume_dist\", cume_dist().over(windowSpec))\n .withColumn(\"lag\",lag(\"salary\",1).over(windowSpec))\n \n .withColumn(\"lead\", lead(\"salary\",1).over(windowSpec))\n .withColumn(\"MINIMUM\",min(\"salary\").over(windowSpec))\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6242d1f-d8f5-48ed-b21e-769291bcf9dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1077835294916387>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cume_dist\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcume_dist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lag\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"max\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpecAgg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lead\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m# previous row\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    566\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 567\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Column is not iterable\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    568\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    569\u001B[0m     \u001B[0;31m# string methods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: Column is not iterable","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: Column is not iterable","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1077835294916387>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cume_dist\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcume_dist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lag\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"max\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpecAgg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"lead\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowSpec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m# previous row\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    566\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 567\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Column is not iterable\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    568\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    569\u001B[0m     \u001B[0;31m# string methods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: Column is not iterable"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Example 2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2298b4f3-e90f-4e66-990d-40c7005c4e1f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_date\ndfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\ndfWithDate.createOrReplaceTempView(\"dfWithDate\")\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\nwindowSpec = Window\\\n    .partitionBy(\"CustomerId\", \"date\")\\\n    .orderBy(desc(\"Quantity\"))\\\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\nfrom pyspark.sql.functions import max\nmaxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n\nfrom pyspark.sql.functions import dense_rank, rank\npurchaseDenseRank = dense_rank().over(windowSpec)\npurchaseRank = rank().over(windowSpec)\n\ndfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n.select(\ncol(\"CustomerId\"),\ncol(\"date\"),\ncol(\"Quantity\"),\npurchaseRank.alias(\"quantityRank\"),\npurchaseDenseRank.alias(\"quantityDenseRank\"),\nmaxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"512dbd4f-0ef5-4db4-ac6a-7d6e62030686","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2413851590457802>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mto_date\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdfWithDate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"date\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mto_date\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"InvoiceDate\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"MM/d/yyyy H:mm\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdfWithDate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateOrReplaceTempView\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dfWithDate\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwindow\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2413851590457802>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mto_date\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdfWithDate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"date\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mto_date\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"InvoiceDate\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"MM/d/yyyy H:mm\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdfWithDate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateOrReplaceTempView\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dfWithDate\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwindow\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 5.4 Grouping Sets"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ffd405ca-7aaa-48a9-9073-fcdceda28fc1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Pivot\nfrom pyspark.sql.functions import to_date, hour\ndfWithDate = df.withColumn('hour', hour(\"InvoiceDate\"))\ndfWithDate.groupBy(\"hour\").pivot(\"Country\").sum(\"Quantity\").orderBy(\"hour\").display()\n\n\npivoted.where(\"hour > '12'\").select(\"hour\" ,\"United Kingdom\", \"Norway\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7392de39-1bad-4fd4-a004-e1bbff6107c3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Rollups (optionnal)\n# A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us\nrolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n  .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n  .orderBy(\"Date\")\nrolledUpDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3547f45a-1534-4d22-accd-924063733af7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1077835294916372>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Rollups\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mrolledUpDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdfNoNull\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrollup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Date\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Country\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0magg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Quantity\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0mselectExpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Date\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Country\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"`sum(Quantity)` as total_quantity\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Date\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'dfNoNull' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'dfNoNull' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1077835294916372>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Rollups\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mrolledUpDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdfNoNull\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrollup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Date\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Country\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0magg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Quantity\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0mselectExpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Date\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Country\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"`sum(Quantity)` as total_quantity\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Date\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'dfNoNull' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["# Cube (optionnal)\n\n# A cube takes the rollup to a level deeper. Rather than treating elements hierarchically, a cube does the same thing across all dimensions\n\nfrom pyspark.sql.functions import sum\ndfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\\\n.select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show(100)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66df376e-ef67-4441-990d-ae2053223f49","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------------+-------------+\n|      Date|        Country|sum(Quantity)|\n+----------+---------------+-------------+\n|      null|        Finland|         1254|\n|      null|      Lithuania|          652|\n|      null|         Poland|          140|\n|      null|        Iceland|          319|\n|      null|        Germany|         6723|\n|      null|         France|         4978|\n|      null|          Italy|          293|\n|      null|          Japan|         4093|\n|      null| United Kingdom|       298101|\n|      null|         Sweden|         3714|\n|      null|        Denmark|          454|\n|      null|           null|       342228|\n|      null|         Cyprus|          917|\n|      null|        Austria|            3|\n|      null|      Australia|          454|\n|      null|         Norway|         3582|\n|      null|           EIRE|         5381|\n|      null|          Spain|          867|\n|      null|        Bahrain|           54|\n|      null|         Israel|          -56|\n|      null|    Switzerland|          714|\n|      null|    Netherlands|         6811|\n|      null|       Portugal|          945|\n|      null|Channel Islands|           80|\n|      null|        Belgium|         1755|\n|2010-12-01|      Australia|          107|\n|2010-12-01| United Kingdom|        23949|\n|2010-12-01|         France|          449|\n|2010-12-01|           null|        26814|\n|2010-12-01|         Norway|         1852|\n|2010-12-01|        Germany|          117|\n|2010-12-01|           EIRE|          243|\n|2010-12-01|    Netherlands|           97|\n|2010-12-02|           null|        21023|\n|2010-12-02| United Kingdom|        20873|\n|2010-12-02|        Germany|          146|\n|2010-12-02|           EIRE|            4|\n|2010-12-03|       Portugal|           65|\n|2010-12-03|           null|        14830|\n|2010-12-03|    Switzerland|          110|\n|2010-12-03|          Spain|          400|\n|2010-12-03| United Kingdom|        10439|\n|2010-12-03|           EIRE|         2575|\n|2010-12-03|         France|          239|\n|2010-12-03|        Germany|          170|\n|2010-12-03|         Poland|          140|\n|2010-12-03|          Italy|          164|\n|2010-12-03|        Belgium|          528|\n|2010-12-05|         France|          611|\n|2010-12-05|          Japan|          196|\n|2010-12-05|        Germany|         1362|\n|2010-12-05|      Lithuania|          622|\n|2010-12-05|           null|        16395|\n|2010-12-05| United Kingdom|        13604|\n|2010-12-06|           EIRE|          614|\n|2010-12-06|           null|        21419|\n|2010-12-06|          Italy|           -2|\n|2010-12-06|        Germany|           85|\n|2010-12-06| United Kingdom|        20669|\n|2010-12-06|       Portugal|           53|\n|2010-12-07| United Kingdom|        23769|\n|2010-12-07|        Iceland|          319|\n|2010-12-07|        Germany|          155|\n|2010-12-07|           null|        24995|\n|2010-12-07|         France|          752|\n|2010-12-08|      Australia|          214|\n|2010-12-08| United Kingdom|        20086|\n|2010-12-08|       Portugal|          424|\n|2010-12-08|           null|        22741|\n|2010-12-08|      Lithuania|           30|\n|2010-12-08|        Germany|          136|\n|2010-12-08|         Norway|         1730|\n|2010-12-08|         France|          121|\n|2010-12-09|           null|        18431|\n|2010-12-09|        Denmark|          454|\n|2010-12-09|         France|         1012|\n|2010-12-09|Channel Islands|           80|\n|2010-12-09|       Portugal|          118|\n|2010-12-09|          Japan|         1488|\n|2010-12-09|        Germany|          380|\n|2010-12-09|           EIRE|          287|\n|2010-12-09| United Kingdom|        14545|\n|2010-12-09|          Spain|           67|\n|2010-12-10| United Kingdom|        18273|\n|2010-12-10|           EIRE|          379|\n|2010-12-10|       Portugal|          131|\n|2010-12-10|         France|          418|\n|2010-12-10|           null|        20297|\n|2010-12-10|        Germany|         1096|\n|2010-12-12|           null|        10565|\n|2010-12-12|          Japan|         2409|\n|2010-12-12| United Kingdom|         8156|\n|2010-12-13|           null|        17623|\n|2010-12-13| United Kingdom|        17325|\n|2010-12-13|        Germany|           -2|\n|2010-12-13|          Spain|          300|\n|2010-12-14|           null|        20098|\n|2010-12-14|      Australia|          -13|\n|2010-12-14|       Portugal|          -39|\n|2010-12-14| United Kingdom|        19233|\n+----------+---------------+-------------+\nonly showing top 100 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 6. Joins\n#### 6.1 Join Types\n\n* Inner joins (keep rows with keys that exist in the left and right datasets)\n* Outer joins (keep rows with keys in either the left or right datasets)\n* Left outer joins (keep rows with keys in the left dataset)\n* Right outer joins (keep rows with keys in the right dataset)\n* Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n* Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\n\n* Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n* Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02a60a62-4842-4ecb-834c-344c507854b0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# paste through codeshare\n\nperson = spark.createDataFrame([\n(0, \"Bill Chambers\", 0, [100]),\n(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n(2, \"Michael Armbrust\", 1, [250, 100])])\\\n.toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\ngraduateProgram = spark.createDataFrame([\n(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n.toDF(\"id\", \"degree\", \"department\", \"school\")\nsparkStatus = spark.createDataFrame([\n(500, \"Vice President\"),\n(250, \"PMC Member\"),\n(100, \"Contributor\")])\\\n.toDF(\"id\", \"status\")\n\nperson.show()\ngraduateProgram.show()\nsparkStatus.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab80ee2d-31b2-4c0f-a395-52a2794b0cce","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------------+----------------+---------------+\n| id|            name|graduate_program|   spark_status|\n+---+----------------+----------------+---------------+\n|  0|   Bill Chambers|               0|          [100]|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|\n|  2|Michael Armbrust|               1|     [250, 100]|\n+---+----------------+----------------+---------------+\n\n+---+-------+--------------------+-----------+\n| id| degree|          department|     school|\n+---+-------+--------------------+-----------+\n|  0|Masters|School of Informa...|UC Berkeley|\n|  2|Masters|                EECS|UC Berkeley|\n|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+-------+--------------------+-----------+\n\n+---+--------------+\n| id|        status|\n+---+--------------+\n|500|Vice President|\n|250|    PMC Member|\n|100|   Contributor|\n+---+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# join key definition\njoinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]\nperson.join(graduateProgram, joinExpression, how = \"left_outer\").show()\n\ngraduateProgram.join(person, joinExpression, how=\"left_semi\").show()\ngraduateProgram.join(person, joinExpression, how=\"left_anti\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0132a629-814e-489d-968b-234fa0187b7c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# join key definition\njoinExpression = person[\"graduate_program\"] == graduateProgram['id']\n\nperson.join(graduateProgram, joinExpression).show() # inner is the default option\n\n\n# \"inner\" \"outer\", \"left_outer\" \"right_outer\" \"left_semi\", \"left_anti\" \"cross\"\nperson.join(graduateProgram, joinExpression, how = \"left_outer\").show()\n\n\n\n# left_semi example : the part for with there is corresponding key from left table\ngraduateProgram.join(person, joinExpression, how=\"left_semi\").show()\n\n# left_anti example : the part for with there is no corresponding key from left table\ngraduateProgram.join(person, joinExpression, how=\"left_anti\").show()\n\n\n\n# person.join(graduateProgram, joinExpression).explain() # on ne voit pas de broadcast en python mais seulement en scala\n# person.rdd.getNumPartitions()\n# graduateProgram.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ab88d85-513c-4ef4-b1eb-fc66531f39fe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [graduate_program#20L], [id#35L], Inner\n   :- Sort [graduate_program#20L ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(graduate_program#20L, 200), ENSURE_REQUIREMENTS, [plan_id=211]\n   :     +- Project [_1#10L AS id#18L, _2#11 AS name#19, _3#12L AS graduate_program#20L, _4#13 AS spark_status#21]\n   :        +- Filter isnotnull(_3#12L)\n   :           +- Scan ExistingRDD[_1#10L,_2#11,_3#12L,_4#13]\n   +- Sort [id#35L ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(id#35L, 200), ENSURE_REQUIREMENTS, [plan_id=212]\n         +- Project [_1#27L AS id#35L, _2#28 AS degree#36, _3#29 AS department#37, _4#30 AS school#38]\n            +- Filter isnotnull(_1#27L)\n               +- Scan ExistingRDD[_1#27L,_2#28,_3#29,_4#30]\n\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 6.2 Joins on Complex Types"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"26093ab2-38e1-4772-ab03-26b2619a99a2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n(person\n .withColumnRenamed('id', 'person_id') # old_name, new_name, you can show first without this line, name is ambigus\n .join(sparkStatus, expr(\"array_contains(spark_status, id)\"))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e932e744-c7ca-4fb8-8324-3f5f4fa45581","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------+----------------+----------------+---------------+---+--------------+\n|personId|            name|graduate_program|   spark_status| id|        status|\n+--------+----------------+----------------+---------------+---+--------------+\n|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n+--------+----------------+----------------+---------------+---+--------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Handling Duplicate Column Names"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e34d9bfc-93b2-4744-babc-b7ecac33b81f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\njoinExpr = person[\"graduate_program\"] == gradProgram3[\"grad_id\"]\nperson.join(gradProgram3, joinExpr).show()\n\nperson.join(gradProgram3, joinExpr).select(\"grad_id\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1715060c-8874-4ee7-939c-5ba55091332b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+\n|grad_id|\n+-------+\n|      0|\n|      1|\n|      1|\n+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\ngradProgramDupe.col[]\"graduate_program\") === person.col(\n\"graduate_program\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e60e47a5-c8e5-44c3-81b4-9b5d337de61d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### How Spark Performs Joins"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"634df6ff-3260-49e4-9b48-e314073cb895","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CEPE_01_Big_data_processing_2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1077835294916348}},"nbformat":4,"nbformat_minor":0}
