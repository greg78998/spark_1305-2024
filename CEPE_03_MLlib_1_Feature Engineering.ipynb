{"cells":[{"cell_type":"markdown","source":["#### Formatting Models According to Your Use Case\n\n* In the case of most classification and regression algorithms, you want to get your data into a column of type Double to represent the label and a column of type Vector (either dense or sparse) to represent the features.\n* In the case of recommendation, you want to get your data into a column of users, a column of items (say movies or books), and a column of ratings.\n* In the case of unsupervised learning, a column of type Vector (either dense or sparse) is needed to represent the features.\n* In the case of graph analytics, you will want a DataFrame of vertices and a DataFrame of edges."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe802421-59a1-445d-b444-1422d7d24d57","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Preprocessing and Feature Engineering"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58a3cf53-e093-45c9-bce9-7b5e510fac79","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 1. Transformers vs estimator\n* Transformers == transform() : 1 seul passage de data\n. estimator == fit_transform() : 2 passages"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"841db5ff-ac26-4c38-95db-cce31e00a01a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["**fit(), transform() and fit_transform()**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf89a337-714d-4bae-a778-662806c878c4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import numpy as np\nfrom sklearn.impute import SimpleImputer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d428a2b4-3bdd-4a72-9530-1c20075cd519","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["X = [[1, 3], \n     [np.nan, 2], \n     [8, 5.5]]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"842026c3-a942-46f5-8521-ea74ff290624","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["imp = SimpleImputer() # par défaut mean(), on peut aussi définir strategy='most_frequent'\n# calculating the means\nimp.fit(X)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec4bb6d3-be9e-47b2-afc0-87b24357edc6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["SimpleImputer()\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Maintenant, les imputateurs ont appris à utiliser une moyenne (1 + 8) / 2 = 4,5 pour la première colonne et une moyenne (2 + 3 + 5,5) / 3 = 3,5 pour la deuxième colonne lorsque celle-ci est appliquée à une donnée à deux colonnes:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec51ec60-850e-4e5f-884d-28c7c636155c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["Y = [[np.nan, 11], \n     [4,      np.nan], \n     [8,      2],\n     [np.nan, 1]]\nimp.transform(Y) # or imp.fit(X).transform(Y)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73de2c30-835c-4a8f-9993-39d3aad9d1d2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[156]: array([[ 4.5, 11. ],\n       [ 4. ,  3.5],\n       [ 8. ,  2. ],\n       [ 4.5,  1. ]])"]}],"execution_count":0},{"cell_type":"markdown","source":["Ainsi, fit l'imputer calcule les moyennes des colonnes à partir de certaines données et les transform applique à certaines données (ce qui remplace simplement les valeurs manquantes par les moyennes). Si ces deux données sont identiques (c'est-à-dire les données permettant de calculer les moyennes et les données auxquelles les moyennes sont appliquées), vous pouvez utiliser fit_transformce qui est fondamentalement un fitsuivi de a transform."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3d21a89-51d9-4c90-b013-8142e754484b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["imp.fit_transform(X) # Deux passages : un pour fit, un pour transform\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e41b1a0-43ea-4ac9-85f0-4fecceb544dc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[157]: array([[1. , 3. ],\n       [4.5, 2. ],\n       [8. , 5.5]])"]}],"execution_count":0},{"cell_type":"code","source":["fakeIntDF = spark.read.parquet(\"/databricks-datasets/definitive-guide/data/simple-ml-integers\")\nsimpleDF = spark.read.json(\"/databricks-datasets/definitive-guide/data/simple-ml\")\nscaleDF = spark.read.parquet(\"/databricks-datasets/definitive-guide/data/simple-ml-scaling\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c549559-fc74-46f2-a872-b3b493e671c8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["scaleDF.show(5)\nsimpleDF.show(5)\nfakeIntDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3794454d-2319-49cb-8968-1b2b2adcfb2c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+------+------------------+\n|color| lab|value1|            value2|\n+-----+----+------+------------------+\n|green|good|     1|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|green|good|    15| 38.97187133755819|\n|green|good|    12|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### 2. Working with Continuous Features\nThere are two common transformers for continuous features\n* bucketing : convert continuous features into categorical features\n* scale and normalize"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb45e724-488b-4d9d-8acd-98e9205038c8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# contDF = spark.range(20).selectExpr(\"cast(id as double)\") # pas besoin de float, en integer marche aussi\n\ncontDF = spark.range(20) \ncontDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"970c34e8-dd87-42d4-b91b-c66efc071639","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+\n|  id|\n+----+\n| 0.0|\n| 1.0|\n| 2.0|\n| 3.0|\n| 4.0|\n| 5.0|\n| 6.0|\n| 7.0|\n| 8.0|\n| 9.0|\n|10.0|\n|11.0|\n|12.0|\n|13.0|\n|14.0|\n|15.0|\n|16.0|\n|17.0|\n|18.0|\n|19.0|\n+----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### 2.1. Bucketing: is a great example to distinct transformer (Bucketizer) and estimator QuantileDiscretizer\n\nMost straightforward approach to bucketing or binning is using the Bucketizer <br/>\nthe values passed into splits must satisfy three requirements:\n* The minimum value in your splits array must be less than the minimum value in your DataFrame\n* The maximum value in your splits array must be greater than the maximum value in your DataFrame\n* need to specify at a minimum **three values** in the splits array, which creates two buckets."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4577bc65-c727-43e7-9d38-6b4974b5f232","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c034434-1215-4d08-8d19-638c03e2a5cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0] # pas besoin de float, en integer marche aussi\nbucketBorders = [-1, 5, 10, 250, 600]\n\nbucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\").setOutputCol(\"class\")\nbucketer.transform(contDF).show()\n\n# poser la question : combien de passage ? estimator ou transfomer?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d31d8276-7d72-4d77-b455-793aee0b2300","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+-------------------------------+\n|  id|Bucketizer_bf495df7fd7f__output|\n+----+-------------------------------+\n| 0.0|                            0.0|\n| 1.0|                            0.0|\n| 2.0|                            0.0|\n| 3.0|                            0.0|\n| 4.0|                            0.0|\n| 5.0|                            1.0|\n| 6.0|                            1.0|\n| 7.0|                            1.0|\n| 8.0|                            1.0|\n| 9.0|                            1.0|\n|10.0|                            2.0|\n|11.0|                            2.0|\n|12.0|                            2.0|\n|13.0|                            2.0|\n|14.0|                            2.0|\n|15.0|                            2.0|\n|16.0|                            2.0|\n|17.0|                            2.0|\n|18.0|                            2.0|\n|19.0|                            2.0|\n+----+-------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import QuantileDiscretizer\nbucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\").setOutputCol(\"class\")\n\n# poser la question : combien de passage ? estimator ou transfomer?\nbucketer.fit(contDF).transform(contDF).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7cefbcf-6e1d-4cb9-80a6-731397d71626","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----------------------------------------+\n|  id|QuantileDiscretizer_d65943e4b48f__output|\n+----+----------------------------------------+\n| 0.0|                                     0.0|\n| 1.0|                                     0.0|\n| 2.0|                                     0.0|\n| 3.0|                                     1.0|\n| 4.0|                                     1.0|\n| 5.0|                                     1.0|\n| 6.0|                                     1.0|\n| 7.0|                                     2.0|\n| 8.0|                                     2.0|\n| 9.0|                                     2.0|\n|10.0|                                     2.0|\n|11.0|                                     3.0|\n|12.0|                                     3.0|\n|13.0|                                     3.0|\n|14.0|                                     3.0|\n|15.0|                                     4.0|\n|16.0|                                     4.0|\n|17.0|                                     4.0|\n|18.0|                                     4.0|\n|19.0|                                     4.0|\n+----+----------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["More advanced techniques such as locality sensitivity hashing (LSH) are also available in MLlib"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb5aee71-bf45-4718-8538-2777db410096","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### 2.2 StandardScaler"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8ddb7f7-6aa6-45e0-9a0c-ac6d0fe5862c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Using scaleDF\nscaleDF.show()\n\nfrom pyspark.ml.feature import StandardScaler\nscale = StandardScaler().setInputCol('features').setOutputCol('features_scaled')\nscale.fit(scaleDF).transform(scaleDF).show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"278efb38-df3a-4485-8e83-5bf8a980ef01","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+------------------------------------------------------------+\n|id |features      |StandardScaler_3efd6830aee9__output                         |\n+---+--------------+------------------------------------------------------------+\n|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n|1  |[3.0,10.1,3.0]|[3.5856858280031805,2.3609991401715313,1.7928429140015902]  |\n+---+--------------+------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### MinMaxScaler (Optional)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37be5978-c57a-4f54-a91b-47f9ebb2eba6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\nminMax = MinMaxScaler().setMin(5).setMax(10).setInputCol('features')\nfittedminMax = minMax.fit(scaleDF)\nfittedminMax.transform(scaleDF).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d58c30e-5d83-45cd-8283-80476cda0a34","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["##### MaxAbsScaler (Optional)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fb24517-9a6b-4036-a65e-8ed7e0d4162a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import MaxAbsScaler"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ec34730-2679-4341-892c-d0dea526c0b8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["maScaler = MaxAbsScaler().setInputCol('features')\nfittedmaScaler= maScaler.fit(scaleDF)\nfittedmaScaler.transform(scaleDF).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eeb56b2b-be9c-4009-87e7-b77fa614b598","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["##### ElementwiseProduct (Optional)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01003734-c30a-4dac-8755-17ef28d5dd94","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import ElementwiseProduct\nfrom pyspark.ml.linalg import Vectors\nscaleUpVec = Vectors.dense(10.0, 15.0, 20.0)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a84ae434-5b5c-4623-bfa0-014bf0503ca9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["scalingUp = ElementwiseProduct().setScalingVec(scaleUpVec).setInputCol('features')\nscalingUp.transform(scaleDF).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57b001ec-707b-4f5e-886c-686cc82756c1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Normalizer (Optional)\nFor any 1 <= p < float(‘inf’), normalizes samples using sum(abs(vector) p) (1/p) as norm <br/>\nNormalization in L^p^ space, p = 2 by default."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78a9574c-9151-465e-8dbe-433f28fa7db9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0577892-7af6-4bf7-8d29-4d08d0579653","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["##### 3. Working with Categorical Features"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d23d5503-ff0c-47cb-b072-2b5d96455906","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### 3.1. StringIndexer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c2f61f8-c040-4cf2-883b-f961fd8af5ac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["simpleDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82a4ad39-0f40-4199-89a0-13a5732843b9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+------+------------------+\n|color| lab|value1|            value2|\n+-----+----+------+------------------+\n|green|good|     1|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|green|good|    15| 38.97187133755819|\n|green|good|    12|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n|  red| bad|     1| 38.97187133755819|\n|  red| bad|     2|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n|  red|good|    45| 38.97187133755819|\n|green|good|     1|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|green|good|    15| 38.97187133755819|\n|green|good|    12|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n|  red| bad|     1| 38.97187133755819|\n|  red| bad|     2|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Using simpleDF\nsimpleDF.show(5)\n\nfrom pyspark.ml.feature import StringIndexer #simplement transformer lab de prédiciton du string en vecteur numérique\n\nstrIndexer = StringIndexer().setInputCol('lab').setOutputCol('lab_Ind')\n\nstrIndexer.fit(simpleDF).transform(simpleDF).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"728bff54-378f-46cf-b9ed-38d9289de81f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+------+------------------+-------+\n|color| lab|value1|            value2|lab_Ind|\n+-----+----+------+------------------+-------+\n|green|good|     1|14.386294994851129|    1.0|\n| blue| bad|     8|14.386294994851129|    0.0|\n| blue| bad|    12|14.386294994851129|    0.0|\n|green|good|    15| 38.97187133755819|    1.0|\n|green|good|    12|14.386294994851129|    1.0|\n|green| bad|    16|14.386294994851129|    0.0|\n|  red|good|    35|14.386294994851129|    1.0|\n|  red| bad|     1| 38.97187133755819|    0.0|\n|  red| bad|     2|14.386294994851129|    0.0|\n|  red| bad|    16|14.386294994851129|    0.0|\n|  red|good|    45| 38.97187133755819|    1.0|\n|green|good|     1|14.386294994851129|    1.0|\n| blue| bad|     8|14.386294994851129|    0.0|\n| blue| bad|    12|14.386294994851129|    0.0|\n|green|good|    15| 38.97187133755819|    1.0|\n|green|good|    12|14.386294994851129|    1.0|\n|green| bad|    16|14.386294994851129|    0.0|\n|  red|good|    35|14.386294994851129|    1.0|\n|  red| bad|     1| 38.97187133755819|    0.0|\n|  red| bad|     2|14.386294994851129|    0.0|\n+-----+----+------+------------------+-------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### Indexing in Vectors (optionnal)\n\nVectorIndexer helps index categorical features in datasets of Vectors. It can both automatically decide which features are categorical and convert original values to category indices. Specifically, it does the following:\n\n- Take an input column of type Vector and a parameter maxCategories.\n- Decide which features should be categorical based on the number of distinct values, where features with at most maxCategories are declared categorical.\n- Compute 0-based category indices for each categorical feature.\n- Index categorical features and transform original feature values to indices.\nIndexing categorical features allows algorithms such as Decision Trees and Tree Ensembles to treat categorical features appropriately, improving performance."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85dc263e-4b52-45f7-a619-dbe829614870","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.linalg import Vectors\n\nidxIn = spark.createDataFrame([\n(Vectors.dense(1, 2, 3),1),\n(Vectors.dense(2, 5, 6),2),\n(Vectors.dense(1, 8, 9),3)\n]).toDF(\"features\", \"label\")\n\n\nidxIn.show()\n\nindxr = VectorIndexer().setInputCol('features').setOutputCol('idxed').setMaxCategories(3) # if == 3, 3 features of vectors are encoded\nindxr.fit(idxIn).transform(idxIn).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ea8dc8d8-dfb6-4ea6-af79-866c7b8180a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+-----+-------------+\n|     features|label|        idxed|\n+-------------+-----+-------------+\n|[1.0,2.0,3.0]|    1|[0.0,0.0,0.0]|\n|[2.0,5.0,6.0]|    2|[1.0,1.0,1.0]|\n|[1.0,8.0,9.0]|    3|[0.0,2.0,2.0]|\n+-------------+-----+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### 3.2. One-Hot Encoding\n**For string type input data, it is common to encode categorical features using StringIndexer first**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"05dc9187-970f-4258-8277-bbaa118eb034","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# important : for string type input data, we have to encode\n\n# Thats why this does not work\n# ohe = OneHotEncoder().setInputCol('color').setOutputCol('ohe_color')\n# ohe.fit(simpleDF).transform(simpleDF).show()\n\n\nfrom pyspark.ml.feature import StringIndexer\nstr_idx = StringIndexer().setInputCol(\"color\").setOutputCol(\"color_idx\")\nsimpleDF_idx = str_idx.fit(simpleDF).transform(simpleDF)\n\nfrom pyspark.ml.feature import OneHotEncoder\nohe = OneHotEncoder().setInputCol('color_idx').setOutputCol('ohe_color')\nohe.fit(simpleDF_idx).transform(simpleDF_idx).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb71cfa6-350a-4bd7-b8a5-0aac2309bd42","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+------+------------------+--------+-------------+\n|color| lab|value1|            value2|colorInd|    color_ohe|\n+-----+----+------+------------------+--------+-------------+\n|green|good|     1|14.386294994851129|     1.0|(2,[1],[1.0])|\n| blue| bad|     8|14.386294994851129|     2.0|    (2,[],[])|\n| blue| bad|    12|14.386294994851129|     2.0|    (2,[],[])|\n|green|good|    15| 38.97187133755819|     1.0|(2,[1],[1.0])|\n|green|good|    12|14.386294994851129|     1.0|(2,[1],[1.0])|\n|green| bad|    16|14.386294994851129|     1.0|(2,[1],[1.0])|\n|  red|good|    35|14.386294994851129|     0.0|(2,[0],[1.0])|\n|  red| bad|     1| 38.97187133755819|     0.0|(2,[0],[1.0])|\n|  red| bad|     2|14.386294994851129|     0.0|(2,[0],[1.0])|\n|  red| bad|    16|14.386294994851129|     0.0|(2,[0],[1.0])|\n|  red|good|    45| 38.97187133755819|     0.0|(2,[0],[1.0])|\n|green|good|     1|14.386294994851129|     1.0|(2,[1],[1.0])|\n| blue| bad|     8|14.386294994851129|     2.0|    (2,[],[])|\n| blue| bad|    12|14.386294994851129|     2.0|    (2,[],[])|\n|green|good|    15| 38.97187133755819|     1.0|(2,[1],[1.0])|\n|green|good|    12|14.386294994851129|     1.0|(2,[1],[1.0])|\n|green| bad|    16|14.386294994851129|     1.0|(2,[1],[1.0])|\n|  red|good|    35|14.386294994851129|     0.0|(2,[0],[1.0])|\n|  red| bad|     1| 38.97187133755819|     0.0|(2,[0],[1.0])|\n|  red| bad|     2|14.386294994851129|     0.0|(2,[0],[1.0])|\n+-----+----+------+------------------+--------+-------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["\"color_ohe\" is represented in sparse format. In this format the zeros of a vector are not printed. \n- The first value (2) shows the length of the vector, \n- the second value is an array that lists zero or more indices where non-zero entries are found. \n- The third value is another array that tells which numbers are found at these indices. \n- So (2,[0],[1.0]) means a vector of length 2 with 1.0 at position 0 and 0 elsewhere (10)\n- 2,[1],[1.0] with 1.0 at position 1 (01)\n- (2,[],[]) means '00'\n\n- 0  -> 10\n- 1  -> 01\n- 2  -> 00"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b24f3f8-d254-4a90-bff8-90383097a4c1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 4. First Pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49c5689f-a287-4ccf-8ee2-5ff07a0c0944","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[\n    StringIndexer(inputCol='lab', outputCol='lab_ind'),\n    StringIndexer(inputCol='color', outputCol='color_idx'),\n    OneHotEncoder(inputCol='color_idx', outputCol='ohe_color'),\n#     IDF(inputCol='a_tf', outputCol='a_idf'),\n])\n\npipeline.fit(simpleDF).transform(simpleDF).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82b21b82-77b5-448c-b57d-c00c41608ad0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If now we'd like to concatenate all numerical features into one big vector to passe to ML algorith, which colums should we choose?\n\n#### 5. VectorAssembler\n* **concatenate all your features into one big vector** you can then pass into an estimator\n* takes as input a number of columns of Boolean, Double, or Vector\n* used in nearly every single pipeline in the last step of a machine learning pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f419751-ec49-449d-ac88-45f6e4e194a9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# VectorAssembler\n\nfrom pyspark.ml.feature import VectorAssembler\nva = VectorAssembler().setInputCols([\"value1\", \"value2\", \"lab_ind\", \"ohe_color\"]).setOutputCol(\"features\")\nva.transform(pipeline_df).show(5, False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"872baab2-232a-43ad-8b23-1df3ed811f46","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# we can also update our pipeline by adding the VectorAssembler()\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=[\n    StringIndexer(inputCol='lab', outputCol='lab_ind'),\n    StringIndexer(inputCol='color', outputCol='color_idx'),\n    OneHotEncoder(inputCol='color_idx', outputCol='ohe_color'),\n    VectorAssembler().setInputCols([\"value1\", \"value2\", \"lab_ind\", \"ohe_color\"]).setOutputCol(\"features\")\n])\n\npipeline.fit(simpleDF).transform(simpleDF).show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c66087c2-c0bd-478f-8cfb-ad1bb40849c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6. Feature Manipulation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e3af591-ebd6-4fc5-b903-8ac6d7cdb595","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### 6.1. PCA"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c04b662-07cd-439b-9a84-bb1768eaf314","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["scaleDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"942d9051-b4f0-4bfd-a2bb-5f99935891b1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+\n| id|      features|\n+---+--------------+\n|  0|[1.0,0.1,-1.0]|\n|  1| [2.0,1.1,1.0]|\n|  0|[1.0,0.1,-1.0]|\n|  1| [2.0,1.1,1.0]|\n|  1|[3.0,10.1,3.0]|\n+---+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import PCA\npca = PCA().setInputCol(\"features\").setK(2)\npca.fit(scaleDF).transform(scaleDF).show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"faa9c445-f232-4a51-9ff9-50b282890fca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+------------------------------------------+\n|id |features      |PCA_c1b7509baacd__output                  |\n+---+--------------+------------------------------------------+\n|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155975]|\n+---+--------------+------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### 6.2 Interaction & Polynomial Expansion\n[Poser la question : transformer or estimator?]\n\nPolynomial(X1, X2, X3) degree 2 = >\nX12 + X22 + X32+  X1 X2 + X2 X3 +X1 X3  + X1 + X2 + X3"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb7589ed-0aac-4bf4-b2bb-a7779020fb9e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["scaleDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93019258-37a7-4277-a1cb-a1df6e0add18","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+\n| id|      features|\n+---+--------------+\n|  0|[1.0,0.1,-1.0]|\n|  1| [2.0,1.1,1.0]|\n|  0|[1.0,0.1,-1.0]|\n|  1| [2.0,1.1,1.0]|\n|  1|[3.0,10.1,3.0]|\n+---+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import PolynomialExpansion\npe = PolynomialExpansion().setInputCol('features').setOutputCol('poly').setDegree(2)\n\n# Poser la question : transformer or estimator?\npe.transform(scaleDF).show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"906b1d5d-f2a3-4b70-aeb6-ad7dce4c2cf6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+-----------------------------------------------------------------------------------+\n|id |features      |PolynomialExpansion_00e44401fba3__output                                           |\n+---+--------------+-----------------------------------------------------------------------------------+\n|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n|1  |[2.0,1.1,1.0] |[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n|1  |[2.0,1.1,1.0] |[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n|1  |[3.0,10.1,3.0]|[3.0,9.0,10.1,30.299999999999997,102.00999999999999,3.0,9.0,30.299999999999997,9.0]|\n+---+--------------+-----------------------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["[,0.1,,a,,,1.0]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc6b379e-49e2-4389-9df2-0887eb77e6f0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 7. High-Level Transformers (optionnal)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43f964ba-8295-4593-b5ce-2d490b509266","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### 7.1. RFormula\nSpark borrows this transformer from the R language to make it simple to declaratively specify a set of transformations for your data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"abc0359f-7526-4dcf-8146-8435be895cea","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\nr_formula = RFormula(formula = 'lab ~.+value1:value2') # je ne comprend pas le resultat\nr_formula.fit(simpleDF).transform(simpleDF).show(2, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e40912e-88fe-48c6-afd5-1abc0bb9493c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+------+------------------+---------------------------------------------------+-----+\n|color|lab |value1|value2            |features                                           |label|\n+-----+----+------+------------------+---------------------------------------------------+-----+\n|green|good|1     |14.386294994851129|[0.0,1.0,1.0,14.386294994851129,14.386294994851129]|1.0  |\n|blue |bad |8     |14.386294994851129|[0.0,0.0,8.0,14.386294994851129,115.09035995880903]|0.0  |\n+-----+----+------+------------------+---------------------------------------------------+-----+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### 7.2. SQL Transformers\n* The only thing you need to change is that instead of using the table name, you should just use the keyword \"__THIS__\"\n* SQLTransformer() is just a transformer whereas that RFormula() is a estimator"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ba63a5d6-837a-4450-ba6a-54126dd650e0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# from pyspark.ml.feature import SQLTransformer\n# basicTransformation = (SQLTransformer()\n#   .setStatement(\"\"\"\n#     select sum(Quantity), count(*), CustomerID\n#     from __THIS__\n#     group by CustomerID\n                       \n#   \"\"\")\n# )\n# basicTransformation.transform(sales).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9db02f97-ccfd-4c29-8ee3-cbdd169dabcf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+--------+----------+\n|sum(Quantity)|count(1)|CustomerID|\n+-------------+--------+----------+\n|          119|      62|   14452.0|\n|          440|     143|   16916.0|\n|          630|      72|   17633.0|\n|           34|       6|   14768.0|\n|         1542|      30|   13094.0|\n+-------------+--------+----------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 8. Persisting Transformers"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2bacfe99-f520-4739-83b4-4fbc85e6b731","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pipeline_fitted = pipeline.fit(simpleDF)\npipeline_fitted.write().overwrite().save(\"/tmp/fitted_Pipeline\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef8450ba-27e2-41de-90d4-22da9a0132ad","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.pipeline import PipelineModel # attention we must import PipelineModel\npipeline_loaded = PipelineModel.load(\"/tmp/fitted_Pipeline\")\n\npipeline_loaded.transform(simpleDF).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"538d28ed-470e-4f36-a881-bb6ae4101837","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import os\nos.chdir('/tmp/')\nos.listdir()\n# os.getcwd()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"31e49c2d-2356-4dd9-88e9-cb63dee3dfc8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[61]: ['hsperfdata_root',\n 'driver-daemon-params',\n 'RtmpemfDFX',\n 'chauffeur-env.sh',\n '.XIM-unix',\n '.X11-unix',\n 'chauffeur-daemon.pid',\n 'systemd-private-a625af562f844c789d618b7595a2bd9f-apache2.service-GdrD3f',\n 'systemd-private-a625af562f844c789d618b7595a2bd9f-ntp.service-mafYzi',\n '.font-unix',\n 'Rserv',\n 'systemd-private-a625af562f844c789d618b7595a2bd9f-systemd-resolved.service-EU5seg',\n 'systemd-private-a625af562f844c789d618b7595a2bd9f-systemd-logind.service-v7Wo8g',\n '.Test-unix',\n 'custom-spark.conf',\n 'driver-daemon.pid',\n 'tmp.FabbD9rQNA',\n 'driver-env.sh',\n '.ICE-unix',\n 'chauffeur-daemon-params']"]}],"execution_count":0},{"cell_type":"code","source":["# from pyspark.ml.feature import PCA\n# pca = PCA().setInputCol(\"features\").setK(2)\n# fittedPCA = pca.fit(scaleDF)\n# fittedPCA.write().overwrite().save(\"/tmp/fittedPCA\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"014da89b-ab62-4784-a227-fea6638080ff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# from pyspark.ml.feature import PCAModel\n# # os.chdir('/tmp/')\n# loadedPCA = PCAModel.load('/tmp/fittedPCA')\n# loadedPCA.transform(scaleDF).show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b0fb935-8128-496e-babb-32f5c2b8f692","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+------------------------------------------+\n|id |features      |PCAModel_8abce6ab4de1__output             |\n+---+--------------+------------------------------------------+\n|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155975]|\n+---+--------------+------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["scaleDF.show()\nfittedPCA.transform(scaleDF).show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f75e063e-4227-4108-9359-7dc75f6cd76a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+\n| id|      features|\n+---+--------------+\n|  0|[1.0,0.1,-1.0]|\n|  1| [2.0,1.1,1.0]|\n|  0|[1.0,0.1,-1.0]|\n|  1| [2.0,1.1,1.0]|\n|  1|[3.0,10.1,3.0]|\n+---+--------------+\n\n+---+--------------+------------------------------------------+\n|id |features      |PCA_ced553f8e30d__output                  |\n+---+--------------+------------------------------------------+\n|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155975]|\n+---+--------------+------------------------------------------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CEPE_03_MLlib_1_Feature Engineering","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4017495545635183}},"nbformat":4,"nbformat_minor":0}
